---
date: "2024-08-07"
title: "What We Learned from a Year of Building with LLMs (Part I)"
slug: "what-we-learned-from-a-year-of-building-with-llms-part-i"
tags: [ ai, llm, tips-and-tricks, rag, prompt ]
---



## Source - [oreilly.com][1]

## Notes
* Prompting
  * Focus on getting the most out of fundamental prompting techniques
  * n-shot prompts & in-context learning
    * If n is too low, the model may over-anchor on those specific examples, hurting its ability to generalize. As a rule of thumb, aim for n ≥ 5. Don’t be afraid to go as high as a few dozen.
    * Examples should be representative of the expected input distribution. If you’re building a movie summarizer, include samples from different genres in roughly the proportion you expect to see in practice.
    * You don’t necessarily need to provide the full input-output pairs. In many cases, examples of desired outputs are sufficient.
    * If you are using an LLM that supports tool use, your n-shot examples should also use the tools you want the agent to use.
  * chain of thoughts
    * Adding specificity via an extra sentence or two often reduces hallucination rates significantly
    *  For example, when asking an LLM to summarize a meeting transcript, we can be explicit about the steps, such as:
      * First, list the key decisions, follow-up items, and associated owners in a sketchpad.
      * Then, check that the details in the sketchpad are factually consistent with the transcript.
      * Finally, synthesize the key points into a concise summary.
  * Structure your inputs and outputs
    * Structured input and output help models better understand the input as well as return output that can reliably integrate with downstream systems
  * Have small prompts that do one thing, and only one thing, well
    * Similar to [KISS][2]
    * If the prompt is doing too many things, you might see performance issues
  * Craft your context tokens
    * Think how much you want the context to be sent along with the prompt. Sending too much context is not going to help
    * Structure your context to underscore the relationships between parts of it, and make extraction as simple as possible
* Information Retrieval/RAG
  * Practitioners have found RAG effective at providing knowledge and improving output, while requiring far less effort and cost compared to fine-tuning
  * The quality of your RAG’s output is dependent on the quality of retrieved documents
  * Relevance
    * Like traditional recommendation systems, the rank of retrieved items will have a significant impact on how the LLM performs on downstream tasks
    * [Mean Reciprocal Rank (MRR)][3] - how well a system places the first relevant result in a ranked list
    * [Normalized Discounted Cumulative Gain (NDCG)][4] - the relevance of all the results and their positions
  * Information Density
    * If two documents are equally relevant, we should prefer one that’s more concise and has lesser extraneous details
  * Level of Detail in the Document
    * A RAG system to generate SQL queries, could generate better SQLs if we provide column descriptions and some representative values, than if we just provide the schema.
  * Don’t forget keyword search; use it as a baseline and in hybrid search
    * If user searches are very limited and have very less context, keyword searches might still perform better than RAG.
    > Vector embeddings **do not** magically solve search. In fact, the heavy lifting is in the step before you re-rank with semantic similarity search. Making a genuine improvement over BM25 or full-text search is hard.
    >
    > — [Aravind Srinivas, CEO Perplexity.ai][5]
    * Keyword search is usually more computationally efficient
    * In most cases, a hybrid will work best: keyword matching for the obvious matches, and embeddings for synonyms, hypernyms, and spelling errors, as well as multimodality (e.g., images and text)
  * Prefer RAG over fine-tuning for new knowledge
    * RAG systems usually out-performed fine-tuned models ([Source 1][6] and [Source 2][7])
    * Other advantages
      * Easier and cheap to keep retrieval indices up to date
      * Problematic (toxic, biased, etc.) documents can be easily dropped
      * Retrieval provides fine grain control over how the documents are retrieved (partitioning, additional indexes, etc.)
  * Long-context models won’t make RAG obsolete
    * We’d still need a way to select information to feed into the model
    * Beyond the narrow needle-in-a-haystack eval, we’ve yet to see convincing data that models can effectively reason over such a large context
    * Cost - Transformer’s inference cost scales quadratically (or linearly in both space and time) with context length
* Tuning and optimizing workflows
  * Step-by-step, multi-turn “flows” can give large boosts
    * Small tasks with clear objectives make for the best agent or flow prompts
  * Prioritize deterministic workflows for now
    * Have agent systems that produce deterministic plans which are then executed in a structured, reproducible way.
    * In the first step, given a high-level goal or prompt, the agent generates a plan.
    * Then, the plan is executed deterministically.
    * This allows each step to be more predictable and reliable.
  * Getting more diverse outputs beyond temperature
    * Adjust the prompt (ask to generate more results)
    * If the prompt has a list of examples, shuffling the list could result in varied responses
    * Keep a list of recently generated responses to compare against. Instruct the LLM to generate from the list or excluded from the list
  * Caching is underrated
    * Generate a unique id for input and cache the response. Any request for the same id, will have response returned from the cache
    * For more open-ended queries, we can borrow techniques from Search.
    * Features like Auto-complete and spelling correction can also help normalize user inputs to increase our cache hit rate.
  * When to fine-tune
    * If prompting gets you 90% of the way there, then fine-tuning may not be worth the investment.
    * If we do decide to fine-tune, to reduce the cost of collecting human annotated data, we can generate and fine-tune on synthetic data, or bootstrap on open-source data.
* Evaluation & Monitoring
  * Create a few assertion-based unit tests from real input/output samples
    * These unit tests, or assertions, should be triggered by any changes to the pipeline, whether it’s editing a prompt, adding new context via RAG, or other modifications.
    * Consider beginning with assertions that specify phrases or ideas to either include or exclude in all responses
    * For code based generations, [Execution-Evaluation][8] is a powerful method
    * Using your product as intended for customers (i.e., ["dogfooding"][10]) can provide insight into failure modes on real-world data
  * LLM-as-Judge can work (somewhat), but it’s not a silver bullet
    * LLM-as-Judge, where we use a strong LLM to evaluate the output of other LLMs
      * Works well if implemented well
    * Use pairwise comparisons - Instead of asking the LLM to score a single output on a [Likert][11] scale, present it with two options and ask it to select the better one. This tends to lead to more stable results
    * Control for position bias - Do each pairwise comparison twice, swapping the order of pairs each time. Just be sure to attribute wins to the right option after swapping!
    * Allow for ties: In some cases, both options may be equally good. Thus, allow the LLM to declare a tie so it doesn’t have to arbitrarily pick a winner.
    * Use Chain-of-Thought: Asking the LLM to explain its decision before giving a final preference can increase eval reliability. As a bonus, this allows you to use a weaker but faster LLM and still achieve similar results. Because frequently this part of the pipeline is in batch mode, the extra latency from CoT isn’t a problem.
    * Control for response length: LLMs tend to bias toward longer responses. To mitigate this, ensure response pairs are similar in length.
  * The "intern test" for evaluating generations
    * If you took the exact input to the language model, including the context, and gave it to an average college student in the relevant major as a task, could they succeed? How long would it take?
      * If the answer is no because the LLM lacks the required knowledge, consider ways to enrich the context.
      * If the answer is no and we simply can’t improve the context to fix it, then we may have hit a task that’s too hard for contemporary LLMs.
      * If the answer is yes, but it would take a while, we can try to reduce the complexity of the task. Is it decomposable? Are there aspects of the task that can be made more templatized?
      * If the answer is yes, they would get it quickly, then it’s time to dig into the data. What’s the model doing wrong? Can we find a pattern of failures? Try asking the model to explain itself before or after it responds, to help you build a theory of mind.
  * Overemphasizing certain evals can hurt overall performance
    * LLMs are so finetuned to attend to every sentence, they may start to treat irrelevant details and distractors as important, thus including them in the final output (when they shouldn’t!)
    * An emphasis on writing style and eloquence could lead to more flowery, marketing-type language that could introduce factual inconsistencies.
  * Simplify annotation to binary tasks or pairwise comparisons
    * In binary classifications, annotators are asked to make a simple yes-or-no judgment on the model’s output
    * In pairwise comparisons, the annotator is presented with a pair of model responses and asked which is better
  * (Reference-free) evals and guardrails can be used interchangeably
    * Reference-free evals can assess the quality of output based solely on the input prompt and the model’s response.
  * LLMs will return output even when they shouldn’t
    * Use content moderation APIs (like OpenAI) to detect harmful content
    * Or [Libraries][11] for these purposes
  * Hallucinations are a stubborn problem.
    *  For prompt engineering, techniques like CoT help reduce hallucination by getting the LLM to explain its reasoning before finally returning the output.
    * When using resources from RAG retrieval, if the output is structured and identifies what the resources are, you should be able to manually verify they’re sourced from the input context.



   [1]: https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/
   [2]: https://en.wikipedia.org/wiki/KISS_principle
   [3]: https://en.wikipedia.org/wiki/Mean_reciprocal_rank
   [4]: https://en.wikipedia.org/wiki/Discounted_cumulative_gain
   [5]: https://x.com/AravSrinivas/status/1737886080555446552
   [6]: https://arxiv.org/abs/2312.05934
   [7]: https://arxiv.org/abs/2401.08406
   [8]: https://www.semanticscholar.org/paper/Execution-Based-Evaluation-for-Open-Domain-Code-Wang-Zhou/1bed34f2c23b97fd18de359cf62cd92b3ba612c3
   [9]: https://en.wikipedia.org/wiki/Likert_scale
  [10]: https://en.wikipedia.org/wiki/Eating_your_own_dog_food
  [11]: https://github.com/topics/pii-detection
