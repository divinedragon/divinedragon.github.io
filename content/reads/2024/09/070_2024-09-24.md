---
date: "2024-09-24"
title: "How streaming LLM APIs work"
slug: "how-streaming-llm-apis-work"
tags: [ ai, llm, programming, streaming, http ]
---



## Source - [simonwillison.net][1]

## Notes
* The response is returned with Header - [`Content-Type: text/event-stream`][2]
* The response is sent in parts/blocks which are split by a sequence of `\r\n\r\n`
* Each block contains information starts with `data:`
* For curl, use the [`--no-buffer`][3] option to see the data as it comes from the Server.
* Example as taken verbatim from [Simon's article][1]

```bash
curl https://api.openai.com/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -d '
    {
      "model": "gpt-4o-mini",
      "messages": [{"role": "user", "content": "Tell me a joke"}],
      "stream": true,
      "stream_options": {
        "include_usage": true
      }
    }' \
    --no-buffer

data: {"id":"chatcmpl-A8dyC7f6pKkQ516qqRHK6ep7Z3yG9","object":"chat.completion.chunk","created":1726623632,"model":"gpt-4o-mini-2024-07-18","system_fingerprint":"fp_483d39d857","choices":[{"index":0,"delta":{"role":"assistant","content":"","refusal":null},"logprobs":null,"finish_reason":null}],"usage":null}

data: {"id":"chatcmpl-A8dyC7f6pKkQ516qqRHK6ep7Z3yG9","object":"chat.completion.chunk","created":1726623632,"model":"gpt-4o-mini-2024-07-18","system_fingerprint":"fp_483d39d857","choices":[{"index":0,"delta":{"content":"Why"},"logprobs":null,"finish_reason":null}],"usage":null}
```



  [1]: https://til.simonwillison.net/llms/streaming-llm-apis
  [2]: https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#sending_events_from_the_server
  [3]: https://curl.se/docs/manpage.html#-N
