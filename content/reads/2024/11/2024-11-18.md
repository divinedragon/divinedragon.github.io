---
date: "2024-11-18"
title: "Universal and Transferable Adversarial Attacks on Aligned Language Models"
slug: "universal-and-transferable-adversarial-attacks-on-aligned-language-models"
tags: [ llm, security, ai, markdown, image, prompt ]
---



## Source - [llm-attacks.org][1]

## Notes
* Demonstrates how a simple adversarial suffix could jailbreak the model to generate content
* Similar paper published from [Imprompter.AI][2]
  * This one demonstrates how rendering a markdown image generation (1x1) passes the PII data in the image url



  [1]: https://llm-attacks.org/index.html
  [2]: https://imprompter.ai/
