---
date: "2024-12-06"
title: "Embeddings are underrated"
slug: "embeddings-are-underrated"
tags: [ ai, llm, embeddings, programming, python ]
---



## Source - [technicalwriting.dev][1]

## Notes
* A detailed explainer for how embeddings work
* The embeddings array represents the point in the n-dimensional space (also referred as a vector)
* Since, each model generate the point in its own way (Gemini's text-embeddings-004 returns a array of 768 numbers, Voyage AI's voyage-3 model returns an array of 1024 numbers), you cannot use embeddings from different providers interchangeably.
* Similar words are placed near to each other in the n-dimensional space
* The concept of positioning items in a multi-dimensional space like this, where related items are clustered near each other, goes by the wonderful name of [latent space][2].
* I kind of like the idea of providing content embeddings as part of well-known URIs. Allows sites (eg. documentation) to be used along with the LLM model of your choice.



   [1]: https://technicalwriting.dev/data/embeddings.html
   [2]: https://en.wikipedia.org/wiki/Latent_space
