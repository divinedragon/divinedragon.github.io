---
date: "2024-12-07"
title: "The Illustrated Word2vec"
slug: "embeddings-are-underrated"
tags: [ ai, llm, embeddings, machine-learning ]
---



## Source - [jalammar.github.io][1]

## Notes
* Even before [ChatGPT][2] became so popular, [Word2vec][3] was at the heart of [Machine Learning][4].
* A very detailed explainer of how a word gets converted to a vector (or point in a n-dimensional space). This process is called [embedding][5] or more precisely [word embedding][5].
* The article is also available as a [YouTube video][6]
* The embeddings array represents the point in the n-dimensional space (also referred as a vector)
* Since, each model generate the point in its own way (Gemini's text-embeddings-004 returns a array of 768 numbers, Voyage AI's voyage-3 model returns an array of 1024 numbers), you cannot use embeddings from different providers interchangeably.
* Similar words are placed near to each other in the n-dimensional space
* The concept of positioning items in a multi-dimensional space like this, where related items are clustered near each other, goes by the wonderful name of [latent space][2].
* I kind of like the idea of providing content embeddings as part of well-known URIs. Allows sites (eg. documentation) to be used along with the LLM model of your choice.
  * Example - /embeddings



   [1]: https://jalammar.github.io/illustrated-word2vec/
   [2]: https://en.wikipedia.org/wiki/ChatGPT
   [3]: https://en.wikipedia.org/wiki/Word2vec
   [4]: https://en.wikipedia.org/wiki/Machine_learning
   [5]: https://en.wikipedia.org/wiki/Word_embedding
   [6]: https://youtu.be/ISPId9Lhc1g
